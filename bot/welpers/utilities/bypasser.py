import re
import time
import urllib.parse
from base64 import standard_b64encode

import cloudscraper
import requests
from bs4 import BeautifulSoup

from bot import Config

# wetransfer
WETRANSFER_API_URL = "https://wetransfer.com/api/v4/transfers"
WETRANSFER_DOWNLOAD_URL = WETRANSFER_API_URL + "/{transfer_id}/download"


def art_j(id):
    scraper = cloudscraper.create_scraper(interpreter="nodejs", allow_brotli=False)
    headers = {
        "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1985.125 Safari/537.36"
    }
    apix = f"https://www.artstation.com/projects/{id}.json"
    response = scraper.get(apix, headers=headers)
    query = response.json()
    return query


def art_k(id):
    uhh = art_j(id)
    text = (
        f'Title : {uhh["title"]}\n\n<a href="{uhh["assets"][0]["image_url"]}">ART ðŸŽ¨</a>'
    )
    return text


def mdis_k(urlx):
    scraper = cloudscraper.create_scraper(interpreter="nodejs", allow_brotli=False)
    headers = {
        "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1985.125 Safari/537.36"
    }
    apix = f"http://x.egraph.workers.dev/?param={urlx}"
    response = scraper.get(apix, headers=headers)
    query = response.json()
    return query


def mdisk(url):
    check = re.findall(r"\bhttps?://.*mdisk\S+", url)
    if not check:
        textx = f"Invalid mdisk url"
        return textx
    else:
        try:
            fxl = url.split("/")
            urlx = fxl[-1]
            uhh = mdis_k(urlx)
            text = f'Title : {uhh["filename"]}\n\n{uhh["download"]}'
            return text
        except ValueError:
            textx = f"The content is deleted."
            return textx


def gplinks(url):
    check = re.findall(r"\bhttps?://.*gplink\S+", url)
    if not check:
        textx = f"Invalid GPLinks url"
        return textx
    else:
        scraper = cloudscraper.create_scraper(interpreter="nodejs", allow_brotli=False)
        res = scraper.get(url)
        h = {"referer": res.url}
        res = scraper.get(url, headers=h)
        bs4 = BeautifulSoup(res.content, "lxml")
        inputs = bs4.find_all("input")
        data = {input.get("name"): input.get("value") for input in inputs}
        h = {
            "content-type": "application/x-www-form-urlencoded",
            "x-requested-with": "XMLHttpRequest",
        }
        time.sleep(10)  # !important
        p = urllib.parse.urlparse(url)
        final_url = f"{p.scheme}://{p.netloc}/links/go"
        res = scraper.post(final_url, data=data, headers=h).json()
        return res["url"]


def droplink(url):
    check = re.findall(r"\bhttps?://.*droplink\S+", url)
    if not check:
        textx = f"Invalid DropLinks url"
        return textx
    else:
        client = requests.Session()
        res = client.get(url)
        ref = re.findall("action[ ]{0,}=[ ]{0,}['|\"](.*?)['|\"]", res.text)[0]
        h = {"referer": ref}
        res = client.get(url, headers=h)
        bs4 = BeautifulSoup(res.content, "lxml")
        inputs = bs4.find_all("input")
        data = {input.get("name"): input.get("value") for input in inputs}
        h = {
            "content-type": "application/x-www-form-urlencoded",
            "x-requested-with": "XMLHttpRequest",
        }
        p = urllib.parse.urlparse(url)
        final_url = f"{p.scheme}://{p.netloc}/links/go"
        time.sleep(3.1)
        res = client.post(final_url, data=data, headers=h).json()
        return res["url"]


def _prepare_session() -> requests.Session:
    """Prepare a wetransfer.com session.
    Return a requests session that will always pass the required headers
    and with cookies properly populated that can be used for wetransfer
    requests.
    """
    s = requests.Session()
    r = s.get("https://wetransfer.com/")
    m = re.search('name="csrf-token" content="([^"]+)"', r.text)
    s.headers.update(
        {
            "x-csrf-token": m.group(1),
            "x-requested-with": "XMLHttpRequest",
        }
    )

    return s


def wetransfer(url):
    """Given a wetransfer.com download URL download return the downloadable URL.
    The URL should be of the form `https://we.tl/' or
    `https://wetransfer.com/downloads/'. If it is a short URL (i.e. `we.tl')
    the redirect is followed in order to retrieve the corresponding
    `wetransfer.com/downloads/' URL.
    The following type of URLs are supported:
     - `https://we.tl/<short_url_id>`:
        received via link upload, via email to the sender and printed by
        `upload` action
     - `https://wetransfer.com/<transfer_id>/<security_hash>`:
        directly not shared in any ways but the short URLs actually redirect to
        them
     - `https://wetransfer.com/<transfer_id>/<recipient_id>/<security_hash>`:
        received via email by recipients when the files are shared via email
        upload
    Return the download URL (AKA `direct_link') as a str or None if the URL
    could not be parsed.
    """
    # Follow the redirect if we have a short URL
    if url.startswith("https://we.tl/"):
        r = requests.head(url, allow_redirects=True)
        url = r.url

    recipient_id = None
    params = urllib.parse.urlparse(url).path.split("/")[2:]

    if len(params) == 2:
        transfer_id, security_hash = params
    elif len(params) == 3:
        transfer_id, recipient_id, security_hash = params
    else:
        return None

    j = {
        "intent": "entire_transfer",
        "security_hash": security_hash,
    }
    if recipient_id:
        j["recipient_id"] = recipient_id
    s = _prepare_session()
    r = s.post(WETRANSFER_DOWNLOAD_URL.format(transfer_id=transfer_id), json=j)

    j = r.json()
    if "direct_link" in j:
        return j["direct_link"]
    else:
        xo = f"The content is expired/deleted."
        return xo


def encod(__str: str) -> str:
    str_bytes = __str.encode("ascii")
    bytes_b64 = standard_b64encode(str_bytes)
    encoded = bytes_b64.decode("ascii")
    return encoded


def bifm(url):
    client = requests.Session()
    headers = {
        "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_4) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/36.0.1985.125 Safari/537.36"
    }
    apix = f"{Config.BIFM_URL}={url}"
    response = client.get(apix, headers=headers)
    query = response.json()
    if "destination" in query:
        return query["destination"]
    else:
        return query["err"]
